{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of articles in each category according to urls_depth_1.json\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"data/urls_depth_3.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "total = 0\n",
    "url_dict = {}\n",
    "for category, urls in data.items():\n",
    "    print(f\"{category}: {len(urls)}\")\n",
    "    total += len(urls)\n",
    "    url_dict[category] = len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[\"Videnskab\"]), len(set(data[\"Videnskab\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = total/3.26\n",
    "min = sec/60\n",
    "hour = min/60\n",
    "print(f\"Total time: {hour} hours, {min} minutes, {sec} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of articles inside each category subfolder\n",
    "import os\n",
    "\n",
    "for category in data.keys():\n",
    "    print(f\"{category}: {len(os.listdir(f'data/wiki_depth_3/{category}'))} / {url_dict[category]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of articles inside each category subfolder\n",
    "import os\n",
    "\n",
    "for category in data.keys():\n",
    "    print(f\"{category}: {len(os.listdir(f'data/wiki_depth_1/{category}'))} / {url_dict[category]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "import html2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Alir3z4/html2text/blob/master/docs/usage.md\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True\n",
    "h.ignore_emphasis = True\n",
    "h.ignore_images = True\n",
    "h.ignore_tables = True\n",
    "h.body_width = 0\n",
    "h.unicode_snob = True\n",
    "h.single_line_break = True\n",
    "\n",
    "def clean_html(raw_text: str):\n",
    "    \"\"\" \n",
    "    Cleans text for html tags \n",
    "    \n",
    "    Args:\n",
    "        raw_text (str): Raw text with html tags\n",
    "    \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    t = h.handle(raw_text) # convert html to text\n",
    "    res = re.findall(r'\\\"(.+?)\\\"', t) # find all text inside quotes\n",
    "    for r in res: # remove unnescessary spaces inside quotes like: \" text \" --> \"text\"\n",
    "        t = t.replace(r, f\"{r.strip()}\")\n",
    "    t = re.sub(\" +\", \" \",t).strip() # remove trailing spaces\n",
    "    t = re.sub(r' ([.,;:?!)}\\]])', r'\\1', t) # remove spaces before punctuations and brackets\n",
    "    t = t.replace(\"*\", \"\").strip() # remove bullet points\n",
    "    t = re.sub(r'\\[.*?\\]', '', t) # remove all inside [] - used for editing wikipage texts (like: [ redigÃ©r | rediger kildetekst] ) + marking links (like: [4])\n",
    "    t = re.sub(r'#', '', t) # avoid #'s that mark headers\n",
    "\n",
    "    lines = [t.strip() for t in t.split(\"\\n\")] # obtain a list of lines\n",
    "    chunks = []\n",
    "    curr_chunk = \"\"\n",
    "    for l in lines: # combine lines into text chunks - discard empty lines\n",
    "        if \"Hentet fra\" in l:\n",
    "            continue\n",
    "        if len(l)>0:\n",
    "            # add punctuations where missing\n",
    "            l = l if l[-1] in [\":\", \";\", \"!\", \"?\", \".\"] else l + \".\" \n",
    "            curr_chunk += \" \" + l\n",
    "        else:\n",
    "            if len(curr_chunk)>0:\n",
    "                chunks.append(curr_chunk.strip())\n",
    "            curr_chunk = \"\"\n",
    "    if len(curr_chunk)>0: # add last text chunk\n",
    "        chunks.append(curr_chunk)\n",
    "    t = \" \".join(chunks )\n",
    "    t = t.replace(\"\\xad\", \"\") # remove soft hyphens (https://en.wikipedia.org/wiki/Soft_hyphen\n",
    "    t = t.replace(\"\\xa0\", \" \") # replace non-breaking space (https://en.wikipedia.org/wiki/Non-breaking_space)\n",
    "    t = t.replace(\"\\u200b\", \"\") # remove zero width space (https://en.wikipedia.org/wiki/Zero-width_space)\n",
    "    t = t.replace(\"\\\\\", \"\")\n",
    "    #t = re.sub(\" +\", \" \",t).strip() # remove trailing spaces\n",
    "    return t.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "texts = []\n",
    "categories = []\n",
    "urls = []\n",
    "for category in tqdm(os.listdir(f\"data/wiki_depth_{depth}\"), desc=\"Loading data\"):\n",
    "    if category == \"Uddannelse\":\n",
    "        for file in os.listdir(f\"data/wiki_depth_{depth}/{category}\"):\n",
    "            with open(f\"data/wiki_depth_{depth}/{category}/{file}\", \"r\") as f:\n",
    "                text_file = f.read()\n",
    "            # get title, text, category and url from text_file (stored using .write(title + \"\\t\" + url + \"\\t\" + category + \"\\t\" + text + \"\\n\"))\n",
    "            titles.append(text_file.split(\"\\t\")[0])\n",
    "            urls.append(text_file.split(\"\\t\")[1])\n",
    "            categories.append(text_file.split(\"\\t\")[2])\n",
    "            texts.append(clean_html(text_file.split(\"\\t\")[3]))\n",
    "data = pd.DataFrame({\"title\": titles, \"text\": texts, \"category\": categories, \"url\": urls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki_scraper",
   "language": "python",
   "name": "wiki_scraper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
